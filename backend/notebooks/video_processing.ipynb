{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db66072d",
   "metadata": {},
   "source": [
    "# Extracting keypoints from videos\n",
    "\n",
    "We will use the following models to extract keypoints:\n",
    "\n",
    "* MoveNet (2D)\n",
    "* MediaPipe Pose Landmarker (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b32ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-06 14:20:30.599377: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-06 14:20:30.859415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-06 14:20:32.599684: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "/home/vilgot/Documents/dev/uni/thesis/Exarbete-5DT90E/.venv/lib/python3.12/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92c2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "MOVENET_KEYPOINT_DICT = {\n",
    "  'nose': 0,\n",
    "  'left_eye': 1,\n",
    "  'right_eye': 2,\n",
    "  'left_ear': 3,\n",
    "  'right_ear': 4,\n",
    "  'left_shoulder': 5,\n",
    "  'right_shoulder': 6,\n",
    "  'left_elbow': 7,\n",
    "  'right_elbow': 8,\n",
    "  'left_wrist': 9,\n",
    "  'right_wrist': 10,\n",
    "  'left_hip': 11,\n",
    "  'right_hip': 12,\n",
    "  'left_knee': 13,\n",
    "  'right_knee': 14,\n",
    "  'left_ankle': 15,\n",
    "  'right_ankle': 16\n",
    "}\n",
    "\n",
    "MEDIAPIPE_JOINTS = [\n",
    "  ('left_shoulder', 11),\n",
    "  ('right_shoulder', 12),\n",
    "  ('left_elbow', 13),\n",
    "  ('right_elbow', 14),\n",
    "  ('left_wrist', 15),\n",
    "  ('right_wrist', 16),\n",
    "  ('left_hip', 23),\n",
    "  ('right_hip', 24),\n",
    "  ('left_knee', 25),\n",
    "  ('right_knee', 26),\n",
    "  ('left_ankle', 27),\n",
    "  ('right_ankle', 28)\n",
    "]\n",
    "\n",
    "# Cropping algorithm to improve detection accuracy (source: https://www.tensorflow.org/hub/tutorials/movenet)\n",
    "\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "  if image_width > image_height:\n",
    "    box_height = image_width / image_height\n",
    "    box_width = 1.0\n",
    "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "    x_min = 0.0\n",
    "  else:\n",
    "    box_height = 1.0\n",
    "    box_width = image_height / image_width\n",
    "    y_min = 0.0\n",
    "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "  return {\n",
    "    'y_min': y_min,\n",
    "    'x_min': x_min,\n",
    "    'y_max': y_min + box_height,\n",
    "    'x_max': x_min + box_width,\n",
    "    'height': box_height,\n",
    "    'width': box_width\n",
    "  }\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "  return ((keypoints[0, 0, MOVENET_KEYPOINT_DICT['left_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, MOVENET_KEYPOINT_DICT['right_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE) and\n",
    "          (keypoints[0, 0, MOVENET_KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, MOVENET_KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(keypoints, target_keypoints, center_y, center_x):\n",
    "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "  max_torso_yrange = 0.0\n",
    "  max_torso_xrange = 0.0\n",
    "  for joint in torso_joints:\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "    if dist_y > max_torso_yrange:\n",
    "      max_torso_yrange = dist_y\n",
    "    if dist_x > max_torso_xrange:\n",
    "      max_torso_xrange = dist_x\n",
    "\n",
    "  max_body_yrange = 0.0\n",
    "  max_body_xrange = 0.0\n",
    "  for joint in MOVENET_KEYPOINT_DICT.keys():\n",
    "    if keypoints[0, 0, MOVENET_KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "      continue\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "    if dist_y > max_body_yrange:\n",
    "      max_body_yrange = dist_y\n",
    "\n",
    "    if dist_x > max_body_xrange:\n",
    "      max_body_xrange = dist_x\n",
    "\n",
    "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "def determine_crop_region(keypoints, image_height, image_width):\n",
    "  target_keypoints = {}\n",
    "  for joint in MOVENET_KEYPOINT_DICT.keys():\n",
    "    target_keypoints[joint] = [\n",
    "      keypoints[0, 0, MOVENET_KEYPOINT_DICT[joint], 0] * image_height,\n",
    "      keypoints[0, 0, MOVENET_KEYPOINT_DICT[joint], 1] * image_width\n",
    "    ]\n",
    "\n",
    "  if torso_visible(keypoints):\n",
    "    center_y = (target_keypoints['left_hip'][0] +\n",
    "                target_keypoints['right_hip'][0]) / 2;\n",
    "    center_x = (target_keypoints['left_hip'][1] +\n",
    "                target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "    (max_torso_yrange, max_torso_xrange,\n",
    "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "          keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "    crop_length_half = np.amax(\n",
    "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "    tmp = np.array(\n",
    "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin(\n",
    "        [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "      return init_crop_region(image_height, image_width)\n",
    "    else:\n",
    "      crop_length = crop_length_half * 2;\n",
    "      return {\n",
    "        'y_min': crop_corner[0] / image_height,\n",
    "        'x_min': crop_corner[1] / image_width,\n",
    "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "        'height': (crop_corner[0] + crop_length) / image_height -\n",
    "            crop_corner[0] / image_height,\n",
    "        'width': (crop_corner[1] + crop_length) / image_width -\n",
    "            crop_corner[1] / image_width\n",
    "      }\n",
    "  else:\n",
    "    return init_crop_region(image_height, image_width)\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "          crop_region['y_max'], crop_region['x_max']]]\n",
    "  output_image = tf.image.crop_and_resize(\n",
    "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "  return output_image\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "  image_height, image_width, _ = image.shape\n",
    "  input_image = crop_and_resize(\n",
    "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "  input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "  \n",
    "  keypoints_with_scores = movenet(input_image)\n",
    "  keypoints_with_scores = keypoints_with_scores['output_0'].numpy()\n",
    "\n",
    "  for idx in range(17):\n",
    "    keypoints_with_scores[0, 0, idx, 0] = (\n",
    "        crop_region['y_min'] * image_height +\n",
    "        crop_region['height'] * image_height *\n",
    "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "    keypoints_with_scores[0, 0, idx, 1] = (\n",
    "        crop_region['x_min'] * image_width +\n",
    "        crop_region['width'] * image_width *\n",
    "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "  return keypoints_with_scores\n",
    "\n",
    "\n",
    "def load_video_data(video_path):\n",
    "  reader = imageio.get_reader(video_path)\n",
    "  fps = reader.get_meta_data()['fps']\n",
    "\n",
    "  frames = []\n",
    "  timestamps = []\n",
    "\n",
    "  for frame_index, frame in enumerate(reader):\n",
    "    frames.append(frame)\n",
    "    timestamps.append(int((frame_index / fps) * 1000))\n",
    "  \n",
    "  return frames, timestamps\n",
    "\n",
    "\n",
    "def load_movenet_model(path):\n",
    "  model = tf_hub.load(path)\n",
    "  model = model.signatures['serving_default']\n",
    "  return model\n",
    "\n",
    "def compute_movenet(frames, model):\n",
    "  frame_height, frame_width, _ = frames[0].shape\n",
    "  input_size = 256\n",
    "  crop_region = init_crop_region(frame_height, frame_width)\n",
    "\n",
    "  keypoint_list = []\n",
    "  for frame in frames:\n",
    "    outputs = run_inference(model, frame, crop_region, crop_size=[input_size, input_size])\n",
    "    keypoint_list.append(outputs)\n",
    "\n",
    "    crop_region = determine_crop_region(outputs, frame_height, frame_width)\n",
    "  \n",
    "  return keypoint_list\n",
    "\n",
    "def restructure_movenet(keypoint_list, timestamps):\n",
    "  stacked = np.concatenate(keypoint_list, axis=0)[:, 0, :, :]\n",
    "  yx = stacked[:, 5:, :] # remove face keypoints\n",
    "\n",
    "  joint_names = [joint for joint, idx in sorted(MOVENET_KEYPOINT_DICT.items(), key=lambda x: x[1]) if idx > 4]\n",
    "  coordinates = {}\n",
    "  for i, joint in enumerate(joint_names):\n",
    "    coordinates[f'{joint}_x'] = yx[:, i, 1]\n",
    "    coordinates[f'{joint}_y'] = yx[:, i, 0]\n",
    "    coordinates[f'{joint}_confidence'] = yx[:, i, 2]\n",
    "  \n",
    "  frames = np.arange(len(keypoint_list), dtype=int)\n",
    "  times = np.asarray(timestamps, dtype=int)\n",
    "\n",
    "  df = pd.DataFrame({'frame': frames, 'time': times, **coordinates})\n",
    "  return df\n",
    "\n",
    "def load_mediapipe_options(path):\n",
    "  base_options = mp.tasks.BaseOptions(model_asset_path=path)\n",
    "  running_mode = mp.tasks.vision.RunningMode.VIDEO\n",
    "  options = mp.tasks.vision.PoseLandmarkerOptions(base_options=base_options, running_mode=running_mode)\n",
    "  return options\n",
    "\n",
    "def compute_mediapipe(frames, timestamps_ms, options):\n",
    "  landmarker_results = []\n",
    "  with mp.tasks.vision.PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    for i in range(len(frames)):\n",
    "      mp_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=frames[i])\n",
    "      timestamp = timestamps_ms[i]\n",
    "      landmarker_result = landmarker.detect_for_video(mp_frame, timestamp)\n",
    "      landmarker_results.append(landmarker_result)\n",
    "  return landmarker_results\n",
    "\n",
    "def restructure_mediapipe(landmarker_results, timestamps, world:bool):\n",
    "  if world:\n",
    "    landmarker_results = [landmarker_result.pose_world_landmarks[0] for landmarker_result in landmarker_results]\n",
    "  else:\n",
    "    landmarker_results = [landmarker_result.pose_landmarks[0] for landmarker_result in landmarker_results]\n",
    "  \n",
    "  stacked = np.stack(landmarker_results)\n",
    "  coordinates = {}\n",
    "  for joint_name, joint_index in MEDIAPIPE_JOINTS:\n",
    "    coordinates[f'{joint_name}_x'] = np.vectorize(lambda joint: joint.x)(stacked[:, joint_index])\n",
    "    coordinates[f'{joint_name}_y'] = np.vectorize(lambda joint: joint.y)(stacked[:, joint_index])\n",
    "    coordinates[f'{joint_name}_z'] = np.vectorize(lambda joint: joint.z)(stacked[:, joint_index])\n",
    "    coordinates[f'{joint_name}_visibility'] = np.vectorize(lambda joint: joint.visibility)(stacked[:, joint_index])\n",
    "    coordinates[f'{joint_name}_presence'] = np.vectorize(lambda joint: joint.presence)(stacked[:, joint_index])\n",
    "  frames = np.arange(len(landmarker_results), dtype=int)\n",
    "  times = np.asarray(timestamps, dtype=int)\n",
    "\n",
    "  df = pd.DataFrame({'frame': frames, 'time': times, **coordinates})\n",
    "  return df\n",
    "\n",
    "\n",
    "def process_video(video_path, movenet_model, mediapipe_options, out_movenet_path, out_mediapipe_norm_path, out_mediapipe_world_path):\n",
    "  frames, timestamps_ms = load_video_data(video_path)\n",
    "  timestamps_sec = [ts // 1000 for ts in timestamps_ms]\n",
    "\n",
    "  movenet_result = compute_movenet(frames, movenet_model)\n",
    "  movenet_structured = restructure_movenet(movenet_result, timestamps_sec)\n",
    "  movenet_structured.to_csv(out_movenet_path, sep=',', index=False)\n",
    "\n",
    "  mediapipe_result = compute_mediapipe(frames, timestamps_ms, mediapipe_options)\n",
    "  mediapipe_norm_structured = restructure_mediapipe(mediapipe_result, timestamps_sec, world=False)\n",
    "  mediapipe_norm_structured.to_csv(out_mediapipe_norm_path, sep=',', index=False)\n",
    "  mediapipe_world_structured = restructure_mediapipe(mediapipe_result, timestamps_sec, world=True)\n",
    "  mediapipe_world_structured.to_csv(out_mediapipe_world_path, sep=',', index=False)\n",
    "\n",
    "  print(f'Video {video_path} processed successfully.')\n",
    "\n",
    "\n",
    "def process_videos(video_dir, movenet_path, mediapipe_path, out_movenet_dir, out_mediapipe_norm_dir, out_mediapipe_world_dir):\n",
    "  Path(out_movenet_dir).mkdir(parents=True, exist_ok=True)\n",
    "  Path(out_mediapipe_norm_dir).mkdir(parents=True, exist_ok=True)\n",
    "  Path(out_mediapipe_world_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "  movenet_model = load_movenet_model(movenet_path)\n",
    "  mediapipe_options = load_mediapipe_options(mediapipe_path)\n",
    "\n",
    "  for video in Path(video_dir).rglob('*.MP4'):\n",
    "    video_name = video.stem\n",
    "    out_movenet_path = Path(out_movenet_dir) / f'{video_name}_movenet.csv'\n",
    "    out_mediapipe_norm_path = Path(out_mediapipe_norm_dir) / f'{video_name}_mediapipe_norm.csv'\n",
    "    out_mediapipe_world_path = Path(out_mediapipe_world_dir) / f'{video_name}_mediapipe_world.csv'\n",
    "    process_video(video, movenet_model, mediapipe_options, out_movenet_path, out_mediapipe_norm_path, out_mediapipe_world_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e03503",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = '../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt'\n",
    "MOVENET_PATH = 'https://tfhub.dev/google/movenet/singlepose/thunder/4'\n",
    "MEDIAPIPE_PATH = '../models/pose_landmarker_heavy.task'\n",
    "\n",
    "OUT_MOVENET_DIR = '../data/processed/keypoints/movenet'\n",
    "OUT_MEDIAPIPE_NORM_DIR = '../data/processed/keypoints/mediapipe_norm'\n",
    "OUT_MEDIAPIPE_WORLD_DIR = '../data/processed/keypoints/mediapipe_world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033be820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770384078.748074   93336 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1770384078.753264   93336 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1770384132.044904   97500 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770384132.103445   97499 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770384132.175936   97495 landmark_projection_calculator.cc:78] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425092743_0028_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770384303.691599   97574 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770384303.903837   97572 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425093100_0030_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770384472.821514   97641 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770384473.478070   97641 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425104507_0045_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770384598.923872   97691 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770384598.986862   97692 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425104804_0047_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770384748.699047   97747 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770384748.745481   97748 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425112502_0059_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770384887.196520   97802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770384887.251077   97802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425112749_0061_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770385034.044623   97854 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770385034.106851   97854 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425120835_0074_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770385148.102555   97907 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770385148.168087   97905 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425121226_0076_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770385277.731349   97967 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770385277.847193   97967 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425125202_0091_D.MP4 processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1770385379.268927   98026 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1770385379.429773   98026 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ../data/Utvalda filminspelningar för IRAF analys/Dec 2025 sit-stå och stå-sitt/DJI_20250425125448_0093_D.MP4 processed successfully.\n"
     ]
    }
   ],
   "source": [
    "process_videos(VIDEO_DIR, MOVENET_PATH, MEDIAPIPE_PATH, OUT_MOVENET_DIR, OUT_MEDIAPIPE_NORM_DIR, OUT_MEDIAPIPE_WORLD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbcf695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, structured_keypoints, joint_names):\n",
    "  height, width, _ = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12*aspect_ratio,12))\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  x = np.array([structured_keypoints[f\"{j}_x\"] for j in joint_names]) * width\n",
    "  y = np.array([structured_keypoints[f\"{j}_y\"] for j in joint_names]) * height\n",
    "  ax.imshow(image)\n",
    "  ax.scatter(x, y, c=\"#00ff00\")\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "  image_from_plot = image_from_plot[:, :, 1:4]\n",
    "  plt.close(fig)\n",
    "\n",
    "  return image_from_plot\n",
    "\n",
    "def draw_keypoints_on_video(frames, structured_keypoints, joint_names, outpath):\n",
    "  images = []  \n",
    "  for i in range(len(frames)):\n",
    "    image = draw_keypoints_on_image(frames[i], structured_keypoints.iloc[i].to_dict(), joint_names)\n",
    "    images.append(image)\n",
    "  \n",
    "  imageio.mimsave(Path(outpath), images, fps=50)\n",
    "\n",
    "POSE_EDGES_12 = [\n",
    "  (11,13),(13,15),    # left arm\n",
    "  (12,14),(14,16),    # right arm\n",
    "  (11,12),            # shoulders\n",
    "  (11,23),(12,24),    # torso\n",
    "  (23,24),            # hips\n",
    "  (23,25),(25,27),    # left leg\n",
    "  (24,26),(26,28),    # right leg\n",
    "]\n",
    "\n",
    "def render_world_frame(world_row):\n",
    "  xs, depth, up = [], [], []\n",
    "  for name, _ in MEDIAPIPE_JOINTS:\n",
    "    xs.append(world_row[f\"{name}_x\"])\n",
    "    depth.append(world_row[f\"{name}_z\"])\n",
    "    up.append(world_row[f\"{name}_y\"])\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "  ax.scatter(xs, depth, up, s=25)\n",
    "\n",
    "  for a, b in POSE_EDGES_12:\n",
    "    ia = [i for i, (_, idx) in enumerate(MEDIAPIPE_JOINTS) if idx == a][0]\n",
    "    ib = [i for i, (_, idx) in enumerate(MEDIAPIPE_JOINTS) if idx == b][0]\n",
    "    ax.plot(\n",
    "      [xs[ia], xs[ib]],\n",
    "      [depth[ia], depth[ib]],\n",
    "      [up[ia], up[ib]],\n",
    "      linewidth=2\n",
    "    )\n",
    "\n",
    "  ax.set_xlim(-0.5, 0.5)\n",
    "  ax.set_ylim(-0.5, 0.5)\n",
    "  ax.set_zlim(-1.0, 1.0)\n",
    "\n",
    "  ax.set_xlabel(\"X\")\n",
    "  ax.set_ylabel(\"Z\")\n",
    "  ax.set_zlabel(\"Y\") # z and y flipped in matplotlib\n",
    "  ax.invert_zaxis()\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  ax.view_init(elev=20, azim=225)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  w, h = fig.canvas.get_width_height()\n",
    "  buf = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "  buf = buf.reshape((h, w, 4))\n",
    "  img = buf[:, :, 1:4]\n",
    "\n",
    "  plt.close(fig)\n",
    "  return img\n",
    "\n",
    "\n",
    "def draw_world_preview_video(world_df, outpath, fps=50):\n",
    "  images = []\n",
    "  for i in range(len(world_df)):\n",
    "    images.append(render_world_frame(world_df.iloc[i]))\n",
    "  imageio.mimsave(outpath, images, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861626e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (2133, 1200) to (2144, 1200) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (2133, 1200) to (2144, 1200) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "joint_names = [name for name, index in MEDIAPIPE_JOINTS]\n",
    "\n",
    "frames, _ = load_video_data(VIDEO_PATH)\n",
    "\n",
    "movenet_structured = pd.read_csv(OUT_MOVENET_PATH)\n",
    "mediapipe_norm_structured = pd.read_csv(OUT_MEDIAPIPE_NORM_PATH)\n",
    "mediapipe_world_structured = pd.read_csv(OUT_MEDIAPIPE_WORLD_PATH)\n",
    "\n",
    "start_frame = 550\n",
    "end_frame = 650\n",
    "\n",
    "draw_keypoints_on_video(frames[start_frame:end_frame], movenet_structured.iloc[start_frame:end_frame], joint_names, '../data/processed/keypoints_on_video/movenet.mp4')\n",
    "draw_keypoints_on_video(frames[start_frame:end_frame], mediapipe_norm_structured.iloc[start_frame:end_frame], joint_names, '../data/processed/keypoints_on_video/mediapipe_norm.mp4')\n",
    "draw_world_preview_video(mediapipe_world_structured.iloc[start_frame:end_frame], \"../data/processed/keypoints_on_video/mediapipe_world.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
